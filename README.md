# ğŸ¤– Chatbot Offline

Este proyecto es una API basada en FastAPI que permite consultar manuales tÃ©cnicos usando modelos LLM locales vÃ­a Ollama y bÃºsqueda semÃ¡ntica con ChromaDB.

## ğŸ› ï¸ Requisitos

- ğŸ Python 3.10+
- ğŸ¦™ Ollama instalado y corriendo en tu mÃ¡quina
- ğŸ§  Modelos LLM compatibles descargados en Ollama (ejemplo: llama3, gemma3:1b, openai/gpt-oss-120b, etc.)
- ğŸ“¦ Dependencias Python (ver `requirements.txt`)

## ğŸš€ InstalaciÃ³n

1. **Clona el repositorio:**
   ```bash
   git clone <URL_DEL_REPO>
   cd chatbot_offline
   ```

2. **Crea y activa un entorno virtual (recomendado):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Instala las dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Descarga los modelos en Ollama:**
   Por ejemplo:
   ```bash
   ollama pull llama3
   ollama pull gemma3:1b
   ollama pull openai/gpt-oss-120b
   ```

5. **Coloca los manuales en la carpeta `documents/`:**
   - ğŸ“„ Archivos PDF y TXT serÃ¡n indexados automÃ¡ticamente.

6. **Genera la base de datos vectorial (opcional al iniciar la API):**
   Puedes ejecutar la ingesta manualmente:
   ```bash
   python ingest.py
   ```
   O al iniciar la API, se te preguntarÃ¡ si deseas ejecutar la ingesta automÃ¡tica:
   ```bash
   python main.py
   ```
   > Se mostrarÃ¡: `Â¿Deseas ejecutar la ingesta automÃ¡tica de documentos? (s/n):`
   > Si respondes "s", se reindexarÃ¡n los documentos antes de iniciar la API.
   Por defecto la API inicia en `http://localhost:8000`

## ğŸ’¡ Uso

### Endpoints principales

#### 1ï¸âƒ£ Chat

`POST /chat`

- Permite conversar con el asistente sobre el contenido de los manuales.
- Mantiene el historial si usas el mismo `session_id`.
- Responde en el idioma solicitado y puedes elegir el modelo LLM.

**Body JSON:**
```json
{
  "question": "Â¿CuÃ¡l es la temperatura recomendada?",
  "session_id": "opcional",
  "language": "es",
  "model": "llama3"
}
```
**Respuesta:**
```json
{
  "answer": "...respuesta generada en el idioma solicitado...",
  "session_id": "..."
}
```

#### 2ï¸âƒ£ Agent

`POST /agent`

- Permite buscar el nombre exacto y valor de una variable en el archivo `model.txt`.
- El modelo razona y responde con un JSON estructurado.

**Body JSON:**
```json
{
  "question": "Â¿CuÃ¡l es el valor actual de ph?",
  "language": "es"
}
```
**Respuesta:**
```json
{
  "variable": "ph.pv",
  "value": 7.2,
  "message": "Valor actual leÃ­do del sistema."
}
```
Si no se encuentra la variable:
```json
{
  "variable": null,
  "value": null,
  "message": "No se encontrÃ³ la variable solicitada."
}
```

### ğŸ§  Memoria de sesiÃ³n
- Si envÃ­as el mismo `session_id` en varias preguntas al chat, el chatbot recordarÃ¡ el historial y responderÃ¡ con contexto.
- Si no envÃ­as `session_id`, el backend generarÃ¡ uno nuevo y lo devolverÃ¡ en la respuesta.

### ğŸŒ Idioma dinÃ¡mico
- Usa el parÃ¡metro `language` para obtener la respuesta en el idioma que desees.
- El modelo intentarÃ¡ responder Ãºnicamente en ese idioma.

### ğŸ”„ Cambiar el modelo LLM
Puedes especificar el modelo en el campo `model` de la peticiÃ³n al chat. Si no lo envÃ­as, se usarÃ¡ el modelo por defecto configurado en el cÃ³digo.

### âš¡ Modularidad y prompts
- Los prompts principales estÃ¡n centralizados en el archivo `llm_utils.py` para facilitar su personalizaciÃ³n y mantenimiento.
- Puedes modificar el comportamiento del asistente editando las funciones de prompt en ese archivo.

### ğŸ“ Notas
- âœ¨ Si cambias los documentos, ejecuta de nuevo la ingesta para reindexar.
- ğŸ¦™ Puedes agregar mÃ¡s modelos a Ollama segÃºn tus necesidades.
- ğŸŒ El sistema soporta preguntas y respuestas en espaÃ±ol, inglÃ©s y otros idiomas.

### ğŸ›¡ï¸ Ingesta interactiva
- Al iniciar la API, se te preguntarÃ¡ si deseas ejecutar la ingesta automÃ¡tica de documentos.
- Esto evita reindexaciones innecesarias y acelera el arranque si no hay cambios en los manuales.

## ğŸ“¬ Contacto
Para dudas o soporte, contacta a Nanotrejo.
