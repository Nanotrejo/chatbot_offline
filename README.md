# ü§ñ Chatbot Offline

Este proyecto es una API basada en FastAPI que permite consultar manuales t√©cnicos usando modelos LLM locales v√≠a Ollama y b√∫squeda sem√°ntica con ChromaDB.

Novedades incluidas en esta rama:
- Par√°metros de chunking centralizados: `CHUNK_SIZE` y `CHUNK_OVERLAP` en `config.py`.
- Historial opcional por sesi√≥n (`use_history`) en los endpoints de chat.
- Reranking opcional con LLM controlado por `ENABLE_RERANK` (configurable en `config.py`).
- Endpoint `/chat_stream` que devuelve respuesta en streaming y registra los fragmentos recuperados en `fragmentos.md` para auditor√≠a.
- Modo `agent` (`/agent`) para extraer nombre/valor de variables desde `model.txt` y devolver un JSON estructurado.
## üõ†Ô∏è Requisitos

- üêç Python 3.10+
- ü¶ô Ollama instalado y corriendo en tu m√°quina
- üß† Modelos LLM compatibles descargados en Ollama (ejemplo: llama3, gemma3:1b, openai/gpt-oss-120b, etc.)
- üì¶ Dependencias Python (ver `requirements.txt`)

## üöÄ Instalaci√≥n

1. **Clona el repositorio:**
   ```bash
   git clone <URL_DEL_REPO>
   cd chatbot_offline
   ```

2. **Crea y activa un entorno virtual (recomendado):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Instala las dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Descarga los modelos en Ollama:**
   Por ejemplo:
   ```bash
   ollama pull llama3
   ollama pull gemma3:1b
   ollama pull openai/gpt-oss-120b
   ```

5. **Coloca los manuales en la carpeta `documents/`:**
   - üìÑ Archivos PDF y TXT ser√°n indexados autom√°ticamente.

6. **Genera la base de datos vectorial (opcional al iniciar la API):**
   Puedes ejecutar la ingesta manualmente:
   ```bash
   python ingest.py
   ```
   O al iniciar la API, se te preguntar√° si deseas ejecutar la ingesta autom√°tica:
   ```bash
   python main.py
   ```
   > Se mostrar√°: `¬øDeseas ejecutar la ingesta autom√°tica de documentos? (s/n):`
   > Si respondes "s", se reindexar√°n los documentos antes de iniciar la API.
   Por defecto la API inicia en `http://localhost:8000`

## üí° Uso

### Endpoints principales

#### 1Ô∏è‚É£ Chat

`POST /chat`

- Permite conversar con el asistente sobre el contenido de los manuales.
- Mantiene el historial si usas el mismo `session_id`.
- Responde en el idioma solicitado y puedes elegir el modelo LLM.
- Mantiene el historial si usas el mismo `session_id` (usa el campo `use_history` para activarlo/desactivarlo por petici√≥n).
- Puedes activar/desactivar el reranker LLM desde `config.py` con `ENABLE_RERANK` (√∫til para ahorrar latencia si no quieres reordenar candidatos con el LLM).

**Body JSON:**
```json
{
  "question": "¬øCu√°l es la temperatura recomendada?",
  "session_id": "opcional",
  "language": "es",
  "model": "llama3"
   "use_history": true
}
```
**Respuesta:**
```json
{
  "answer": "...respuesta generada en el idioma solicitado...",
  "session_id": "..."
}
```

#### 2Ô∏è‚É£ Agent

`POST /agent`

- Permite buscar el nombre exacto y valor de una variable en el archivo `model.txt`.
- El modelo razona y responde con un JSON estructurado.

**Body JSON:**
```json
{
  "question": "¬øCu√°l es el valor actual de ph?",
  "language": "es"
}
```
**Respuesta:**
```json
{
  "variable": "ph.pv",
  "value": 7.2,
  "message": "Valor actual le√≠do del sistema."
}
```
Si no se encuentra la variable:
```json
{
  "variable": null,
  "value": null,
  "message": "No se encontr√≥ la variable solicitada."
}
```

### üß† Memoria de sesi√≥n
- Si env√≠as el mismo `session_id` en varias preguntas al chat, el chatbot recordar√° el historial y responder√° con contexto.
- Si no env√≠as `session_id`, el backend generar√° uno nuevo y lo devolver√° en la respuesta.
 - Controla la memoria por petici√≥n con `use_history: true|false` en el body.

### üåé Idioma din√°mico
- Usa el par√°metro `language` para obtener la respuesta en el idioma que desees.
- El modelo intentar√° responder √∫nicamente en ese idioma.

### üîÑ Cambiar el modelo LLM
Puedes especificar el modelo en el campo `model` de la petici√≥n al chat. Si no lo env√≠as, se usar√° el modelo por defecto configurado en el c√≥digo.

### ‚ö° Modularidad y prompts
- Los prompts principales est√°n centralizados en el archivo `llm_utils.py` para facilitar su personalizaci√≥n y mantenimiento.
- Puedes modificar el comportamiento del asistente editando las funciones de prompt en ese archivo.

## üìÅ Estructura del proyecto y qu√© hace cada archivo
- `main.py` ‚Äî API FastAPI: inicia la app, carga ChromaDB, configura el retriever y define los endpoints principales (`/chat`, `/chat_stream`, `/agent`). Tambi√©n contiene la funci√≥n `rerank_with_llm` usada para reordenar fragmentos por relevancia.
- `config.py` ‚Äî Variables de configuraci√≥n globales: modelos, rutas, par√°metros de retriever y ahora los par√°metros para fragmentaci√≥n (`CHUNK_SIZE`, `CHUNK_OVERLAP`). Modifica este archivo para ajustar comportamiento global.
- `db_utils.py` ‚Äî L√≥gica de ingesti√≥n y chunking de documentos: carga PDFs/TXT y los divide en fragmentos (`Document`). Ahora toma `CHUNK_SIZE`, `CHUNK_OVERLAP` y `DATA_DIR` desde `config.py`.
- `llm_utils.py` ‚Äî Prompts centralizados y utilidades de parsing. Contiene los prompts para modo agente (`build_agent_prompt`), chat (`build_chat_prompt`), manuales (`build_manual_prompt`) y helper/Biro (`build_gpt_helper_prompt`), m√°s la funci√≥n `extract_json_from_response`.
- `ingest.py` ‚Äî (si existe) script para ingesta independiente de documentos (√∫til para pipelines o CI).
- `documents/` ‚Äî Carpeta donde colocas PDFs y TXT que quieres indexar.
- `chroma_db/` ‚Äî Directorio donde Chroma persiste vectores.
- `fragmentos.md` ‚Äî Archivo de registro donde el endpoint `/chat_stream` a√±ade los fragmentos recuperados en cada consulta (√∫til para depuraci√≥n y auditor√≠a).

Notas sobre `fragmentos.md`:
- El endpoint escribe los fragmentos en modo append (`a`) para conservar historial de consultas y facilitar auditor√≠a.
- Si quieres que se sobrescriba (recrear el archivo con s√≥lo los fragmentos de la consulta actual), el c√≥digo se puede cambiar para abrir el archivo en modo `w` antes de escribir.
- Para evitar escrituras parciales o condiciones de carrera en entornos concurrentes, recomendamos escribir de forma at√≥mica usando un fichero temporal y `os.replace()` o usar un lock (`filelock`).

Ejemplo de escritura at√≥mica en el c√≥digo:
```python
import tempfile, os

tmp_path = None
with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
   tmp_path = tmp.name
   for doc in docs_relevantes:
      tmp.write(f"### Fragmento encontrado\n\n{getattr(doc, 'page_content', str(doc))}\n\n")
os.replace(tmp_path, "fragmentos.md")
```

## ‚öôÔ∏è Par√°metros importantes en `config.py`
- `DATA_DIR`: carpeta de documentos a indexar (por defecto `documents/`).
- `CHUNK_SIZE`: tama√±o en caracteres de cada fragmento (ej. 1200). Ajusta para controlar cu√°nto contexto aporta cada fragmento.
- `CHUNK_OVERLAP`: solapamiento entre fragmentos (ej. 200). √ötil para evitar cortar informaci√≥n cr√≠tica entre fragments.
- `EMBEDDING_MODEL`: modelo usado para generar embeddings.
- `LLM_MODEL`: modelo usado por Ollama para respuestas.
- `RETRIEVER_K`, `RETRIEVER_FETCH_K`, `TOP_K_RERANK`: configuraci√≥n del retriever y reranking.
- `ENABLE_RERANK`: booleano que activa/desactiva la etapa de reranking realizada por el LLM. √ötil para controlar latencia.

## üß≠ C√≥mo inspeccionar qu√© guarda la base de datos y qu√© fragmentos se usan
- El endpoint `/chat_stream` a√±ade los fragmentos que recupera desde ChromaDB a `fragmentos.md`. Abre ese archivo para ver los fragmentos exactos (texto y orden) que se usan para producir respuestas.
- Para una inspecci√≥n m√°s directa puedes reusar `db_utils.load_all_documents()` y crear un peque√±o script que imprima `doc.metadata` y las primeras l√≠neas de `doc.page_content`.

Ejemplo r√°pido para listar fragmentos en Python:
```python
from db_utils import load_all_documents

docs = load_all_documents()
for i, d in enumerate(docs[:50]):
   print(i, d.metadata.get('source'), '---', d.page_content[:200].replace('\n',' '))
```

## üîß Consejos para mejorar la precisi√≥n al responder usando solo documentos indexados
- Ajusta `CHUNK_SIZE` y `CHUNK_OVERLAP` en `config.py` y reindexa. Para manuales t√©cnicos, valores como `1200/200` suelen funcionar bien.
- Usa un embeding multiling√ºe si trabajas con varios idiomas.
- Configura el retriever para recuperar m√°s candidatos (`fetch_k`) y luego rerankear (`TOP_K_RERANK`).
- Refuerza los prompts en `llm_utils.py` para obligar al modelo a usar SOLO el contexto; ya hay prompts que devuelven respuestas concisas y piden citar documento/p√°gina.

## üõ†Ô∏è Reindexar / actualizar la base de datos
- Para reindexar manualmente: elimina `chroma_db/` o la ruta indicada por `DB_PATH` y ejecuta el script de ingesta o inicia `main.py` y responde "s" cuando pregunte por la ingesta autom√°tica.
- Si cambias el `EMBEDDING_MODEL` debes reindexar todo para regenerar vectores.

## üßæ Feedback y mejora continua
- Actualmente no hay un endpoint oficial de feedback en el repo. Recomendaci√≥n r√°pida: crea un endpoint `/feedback` que guarde entradas en `feedback.jsonl` (acci√≥n: accept/reject/correct). Las correcciones pueden convertirse en documentos que luego se reindexan en lote.
- Otra opci√≥n inmediata: a√±ade manualmente archivos a `documents/` con correcciones y reindexa.

## ‚úÖ Qu√© probar ahora
1. A√±ade tus documentos en `documents/` y ejecuta `python main.py`.
2. Responde "s" para crear la base vectorial si quieres reindexar.
3. Prueba `POST /chat` y `POST /chat_stream` desde `http://localhost:8000/docs`.
4. Abre `fragmentos.md` para ver los fragmentos que se recuperaron para cada consulta.

## üì¨ Contacto
Para dudas o soporte, contacta a Nanotrejo.

### üìù Notas
- ‚ú® Si cambias los documentos, ejecuta de nuevo la ingesta para reindexar.
- ü¶ô Puedes agregar m√°s modelos a Ollama seg√∫n tus necesidades.
- üåé El sistema soporta preguntas y respuestas en espa√±ol, ingl√©s y otros idiomas.

### üõ°Ô∏è Ingesta interactiva
- Al iniciar la API, se te preguntar√° si deseas ejecutar la ingesta autom√°tica de documentos.
- Esto evita reindexaciones innecesarias y acelera el arranque si no hay cambios en los manuales.

## üì¨ Contacto
Para dudas o soporte, contacta a Nanotrejo.
