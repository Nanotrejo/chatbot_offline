# ğŸ¤– Chatbot Offline - Manual Rosita

Este proyecto es una API basada en FastAPI que permite consultar manuales tÃ©cnicos usando modelos LLM locales vÃ­a Ollama y bÃºsqueda semÃ¡ntica con ChromaDB.

## ğŸ› ï¸ Requisitos

- ğŸ Python 3.10+
- ğŸ¦™ Ollama instalado y corriendo en tu mÃ¡quina
- ğŸ§  Modelos LLM compatibles descargados en Ollama (ejemplo: llama3, gemma3:1b, openai/gpt-oss-120b, etc.)
- ğŸ“¦ Dependencias Python (ver `requirements.txt`)

## ğŸš€ InstalaciÃ³n

1. **Clona el repositorio:**
   ```bash
   git clone <URL_DEL_REPO>
   cd chatbot_offline
   ```

2. **Crea y activa un entorno virtual (recomendado):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Instala las dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Descarga los modelos en Ollama:**
   Por ejemplo:
   ```bash
   ollama pull llama3
   ollama pull gemma3:1b
   ollama pull openai/gpt-oss-120b
   ```

5. **Coloca los manuales en la carpeta `documents/`:**
   - ğŸ“„ Archivos PDF y TXT serÃ¡n indexados automÃ¡ticamente.

6. **Genera la base de datos vectorial:**
   ```bash
   python ingest.py
   ```

7. **Inicia la API:**
   ```bash
   python main.py
   ```
   Por defecto se inicia en `http://localhost:8000`

## ğŸ’¡ Uso

### Endpoint principal

`POST /chat`

**Body JSON:**
```json
{
  "question": "Â¿CuÃ¡l es la temperatura recomendada?",
  "session_id": "opcional",
  "language": "es",
  "model": "llama3" // Puedes cambiar el modelo aquÃ­
}
```

**Respuesta:**
```json
{
  "answer": "...respuesta generada...",
  "session_id": "..."
}
```

### ğŸ”„ Cambiar el modelo LLM
Puedes especificar el modelo en el campo `model` de la peticiÃ³n. Si no lo envÃ­as, se usarÃ¡ el modelo por defecto configurado en el cÃ³digo.

## ğŸ“ Notas
- âœ¨ Si cambias los documentos, ejecuta de nuevo `python ingest.py` para reindexar.
- ğŸ¦™ Puedes agregar mÃ¡s modelos a Ollama segÃºn tus necesidades.
- ğŸŒ El sistema soporta preguntas en espaÃ±ol y otros idiomas.

## ğŸ“¬ Contacto
Para dudas o soporte, contacta a Nanotrejo.
