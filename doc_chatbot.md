# ğŸ“š ExplicaciÃ³n de componentes y argumentos del chatbot

## ğŸ”‘ Argumentos principales
- **question**: Pregunta del usuario que se envÃ­a al chatbot.
- **session_id**: Identificador de la sesiÃ³n para mantener el historial de la conversaciÃ³n.
- **language**: Idioma en el que se solicita la respuesta.
- **model**: Permite especificar el modelo LLM a utilizar (por ejemplo, "llama3", "phi3:mini").

## ğŸ¤– LLM (Large Language Model)
- Es el modelo de lenguaje que genera las respuestas del chatbot.
- Se conecta a travÃ©s de Ollama y puede ser cambiado segÃºn el parÃ¡metro `model`.
- Ejemplo de modelos: llama3, phi3:mini, gemma3:4b.

## ğŸ—ƒï¸ Base de datos vectorial (ChromaDB)
- Almacena los fragmentos de los documentos en forma de vectores para realizar bÃºsquedas semÃ¡nticas.
- Permite encontrar el contexto relevante para cada pregunta.
- Se crea a partir de los documentos PDF/TXT usando embeddings.

## ğŸ§© Embeddings
- Modelo que convierte el texto en vectores numÃ©ricos para la bÃºsqueda semÃ¡ntica.
- Ejemplo: "sentence-transformers/paraphrase-mpnet-base-v2".
- Debe ser el mismo tanto en la ingesta como en la consulta.

---

## âš™ï¸ ParÃ¡metros clave

### k (nÃºmero de fragmentos relevantes a recuperar)
- Recomendado: **3-8** para precisiÃ³n y contexto suficiente.
- Si el contexto es muy disperso, puedes subirlo a **10-15**.
- ğŸ”¼ Si recibes muchas respuestas "No tengo informaciÃ³n...", sube k.
- ğŸ”½ Si el modelo responde con informaciÃ³n irrelevante, baja k.

### temperature (creatividad del modelo)
- Recomendado: **0.0 a 0.2** para respuestas tÃ©cnicas y precisas.
- Si quieres respuestas mÃ¡s creativas, sube a **0.5 o mÃ¡s**.
- Para manuales y soporte tÃ©cnico, lo Ã³ptimo es **0.1**.

---

## ğŸ§± Chunking (fragmentaciÃ³n de documentos)
- **chunk_size**: TamaÃ±o de cada fragmento (recomendado: **1200**).
- **chunk_overlap**: Solapamiento entre fragmentos (recomendado: **200**).
- Si el chunk es muy pequeÃ±o, puedes perder contexto relevante.
- Si es muy grande, la bÃºsqueda puede ser menos precisa y mÃ¡s lenta.

---

## ğŸ† Modelos de embeddings recomendados

1. **paraphrase-multilingual-MiniLM-L12-v2**
   - ğŸŒ MultilingÃ¼e, rÃ¡pido, buen rendimiento general.
   - âš ï¸ Puede perder matices tÃ©cnicos muy especÃ­ficos.
2. **all-MiniLM-L6-v2**
   - âš¡ Muy eficiente, buena precisiÃ³n en inglÃ©s y espaÃ±ol.
   - âš ï¸ Menos potente para textos largos o muy tÃ©cnicos.
3. **sentence-transformers/paraphrase-mpnet-base-v2**
   - ğŸ§  Mejor comprensiÃ³n semÃ¡ntica, mÃ¡s robusto para textos complejos.
   - âš ï¸ MÃ¡s pesado, requiere mÃ¡s recursos.
4. **distiluse-base-multilingual-cased-v2**
   - ğŸŒ MultilingÃ¼e, buen equilibrio entre velocidad y calidad.
   - âš ï¸ Menos preciso que los modelos MPNet.

---

## ğŸ“ Recomendaciones generales
- ğŸ”¼ Aumenta el valor de k en KWARGS si necesitas mÃ¡s contexto.
- ğŸ”„ Prueba con otro modelo de embeddings si no encuentras resultados precisos.
- ğŸ§± Ajusta el tamaÃ±o de los fragmentos (chunk_size y chunk_overlap).
- ğŸ” Cambia el tipo de bÃºsqueda a "mmr" o "similarity_score_threshold" si la pregunta es ambigua.
- âœ… Verifica que los documentos estÃ©n bien indexados y el texto sea legible.

---

## ğŸ“Š Diferencias entre `/chat` y `/agent`
- **/chat**: Responde preguntas generales sobre los documentos, usando el modelo LLM y la base de datos vectorial completa.
- **/agent**: Busca y razona sobre variables tÃ©cnicas en el archivo `model.txt`, devolviendo el nombre exacto, valor y mensaje en formato JSON.

---

## ğŸ”¬ EMBEDDING_MODEL
- Especifica el modelo de embeddings que convierte el texto en vectores numÃ©ricos para la bÃºsqueda semÃ¡ntica.
- Ejemplo de valor: "sentence-transformers/paraphrase-mpnet-base-v2".
- Es fundamental que el mismo modelo se use tanto para la ingesta de documentos como para la consulta, para que los vectores sean compatibles y la bÃºsqueda sea precisa.
- Cambiar el modelo puede afectar la calidad y el idioma de la bÃºsqueda semÃ¡ntica.

---
